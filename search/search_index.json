{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Whisper ASR Webservice","text":"<p>Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitask model that can perform multilingual speech recognition as well as speech translation and language identification.</p>"},{"location":"#features","title":"Features","text":"<p>Current release (v1.2.0) supports following whisper models:</p> <ul> <li>openai/whisper@v20230918</li> <li>guillaumekln/faster-whisper@0.9.0</li> </ul>"},{"location":"build/","title":"Build","text":""},{"location":"build/#development-environment","title":"Development Environment","text":"<p>Install poetry with following command:</p> <pre><code>pip3 install poetry\n</code></pre> <p>Install torch with following command:</p> <pre><code># just for GPU:\npip3 install torch==1.13.0+cu117 -f https://download.pytorch.org/whl/torch\n</code></pre>"},{"location":"build/#run","title":"Run","text":"<p>Install packages:</p> <pre><code>poetry install\n</code></pre> <p>Starting the Webservice:</p> <pre><code>poetry run gunicorn --bind 0.0.0.0:9000 --workers 1 --timeout 0 app.webservice:app -k uvicorn.workers.UvicornWorker\n</code></pre>"},{"location":"build/#build","title":"Build","text":"<code>Poetry</code> <code>Docker</code> <p>Build .whl package</p> <pre><code>poetry build\n</code></pre> <p>With <code>Dockerfile</code>:</p> <code>CPU</code> <code>GPU</code> <pre><code># Build Image\ndocker build -t whisper-asr-webservice .\n\n# Run Container\ndocker run -d -p 9000:9000 whisper-asr-webservice\n# or\ndocker run -d -p 9001:9000 -e ASR_MODEL=base whisper-asr-webservice3\n</code></pre> <pre><code># Build Image\ndocker build -f Dockerfile.gpu -t whisper-asr-webservice-gpu .\n\n# Run Container\ndocker run -d --gpus all -p 9000:9000 whisper-asr-webservice-gpu\n# or\ndocker run -d --gpus all -p 9000:9000 -e ASR_MODEL=base whisper-asr-webservice-gpu\n</code></pre> <p>With <code>docker-compose</code>:</p> <code>CPU</code> <code>GPU</code> <pre><code>docker-compose up --build\n</code></pre> <pre><code>docker-compose up --build -f docker-compose.gpu.yml\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#120-2023-10-01","title":"1.2.0 (2023-10-01)","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Upgraded<ul> <li>openai/whisper to v20230918</li> <li>guillaumekln/faster-whisper to v0.9.0</li> </ul> </li> </ul>"},{"location":"changelog/#updated","title":"Updated","text":"<ul> <li>Updated model conversion method (for Faster Whisper) to use Hugging Face downloader</li> <li>Updated default model paths to <code>~/.cache/whisper</code>.<ul> <li>For customization, modify the <code>ASR_MODEL_PATH</code> environment variable.</li> <li>Ensure Docker volume is set for the corresponding directory to use caching.   <pre><code>docker run -d -p 9000:9000 -e ASR_MODEL_PATH=/data/whisper -v ./yourlocaldir:/data/whisper onerahmet/openai-whisper-asr-webservice:latest\n</code></pre></li> </ul> </li> <li>Removed the <code>triton</code> dependency from <code>poetry.lock</code> to ensure the stability of the pipeline for <code>ARM-based</code> Docker images</li> </ul>"},{"location":"changelog/#111-2023-05-29","title":"1.1.1 (2023-05-29)","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>94 gpus that don't support float16 in #103</li> <li>Update compute type in #108</li> <li>Add word level functionality for Faster Whisper in #109</li> </ul>"},{"location":"changelog/#110-2023-04-17","title":"1.1.0 (2023-04-17)","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Docs in #72</li> <li>Fix language code typo in #77</li> <li>Adds support for FasterWhisper in #81</li> <li>Add an optional param to skip the encoding step in #82</li> <li>Faster whisper in #92</li> </ul>"},{"location":"changelog/#106-2023-02-05","title":"1.0.6 (2023-02-05)","text":""},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Update README.md in #58</li> <li>68 update the versions in #69</li> <li>Fix gunicorn run command and remove deprecated poetry run script in #70</li> <li>Move torch installation method into the pyproject.toml file in #71</li> <li>Add prompt to ASR in #66</li> </ul>"},{"location":"changelog/#105-2022-12-08","title":"1.0.5 (2022-12-08)","text":""},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>43 make swagger doc not depend on internet connection in #52</li> <li>Add new large model v2 in #53</li> </ul>"},{"location":"changelog/#104-2022-11-28","title":"1.0.4 (2022-11-28)","text":""},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>43 make swagger doc not depend on internet connection in #51</li> <li>Anally retentively fixed markdown linting warnings in README. Sorry. in #48</li> <li>Explicit macOS readme with explanation for no-GPU [closes #44] in #47</li> </ul>"},{"location":"changelog/#103-beta-2022-11-17","title":"1.0.3-beta (2022-11-17)","text":""},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Combine transcribe endpoints in #36</li> <li>Add multi worker support with gunicorn in #37</li> <li>Add multi platform (amd &amp; arm) support in #39</li> <li>Upgrade Cuda version to 11.7 in #40</li> <li>Lock to the latest whisper version (eff383) in #41</li> </ul>"},{"location":"changelog/#102-beta-2022-10-04","title":"1.0.2-beta (2022-10-04)","text":""},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>add mutex lock to the model in #19</li> <li>Subtitles in #21</li> <li>Add gpu support and create Docker image for cuda with GitHub flow in #22</li> </ul>"},{"location":"changelog/#101-beta-2022-09-27","title":"1.0.1-beta (2022-09-27)","text":""},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Init GitHub runners in #10</li> <li>Lock Whisper dependency with b4308... revision number to prevent build crashes in #15</li> </ul>"},{"location":"changelog/#100-beta-2022-09-25","title":"1.0.0-beta (2022-09-25)","text":""},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Docker init in #1</li> <li>Create LICENCE in #2</li> <li>Fastapi init in #3</li> <li>Avoid temp file in #4</li> <li>Translate init in #5</li> <li>mp3 support by using ffmpeg instead of librosa in #8</li> <li>add language detection endpoint in #9</li> </ul>"},{"location":"endpoints/","title":"Endpoints","text":""},{"location":"endpoints/#quick-start","title":"Quick start","text":"<p>After running the docker image interactive Swagger API documentation is available at localhost:9000/docs</p> <p>There are 2 endpoints available:</p> <ul> <li>/asr (TXT, VTT, SRT, TSV, JSON)</li> <li>/detect-language</li> </ul>"},{"location":"endpoints/#automatic-speech-recognition-service-asr","title":"Automatic Speech recognition service /asr","text":"<p>If you choose the transcribe task, transcribes the uploaded file. Both audio and video files are supported (as long as ffmpeg supports it).</p> <p>Note that you can also upload video formats directly as long as they are supported by ffmpeg.</p> <p>You can get TXT, VTT, SRT, TSV and JSON output as a file from /asr endpoint.</p> <p>You can provide the language or it will be automatically recognized.</p> <p>If you choose the translate task it will provide an English transcript no matter which language was spoken.</p> <p>You can enable word level timestamps output by <code>word_timestamps</code> parameter (only with <code>Faster Whisper</code> for now).</p> <p>Returns a json with following fields:</p> <ul> <li>text: Contains the full transcript</li> <li>segments: Contains an entry per segment. Each entry provides <code>timestamps</code>, <code>transcript</code>, <code>token ids</code>, <code>word level timestamps</code> and other metadata</li> <li>language: Detected or provided language (as a language code)</li> </ul>"},{"location":"endpoints/#language-detection-service-detect-language","title":"Language detection service /detect-language","text":"<p>Detects the language spoken in the uploaded file. For longer files it only processes first 30 seconds.</p> <p>Returns a json with following fields:</p> <ul> <li>detected_language</li> <li>language_code</li> </ul>"},{"location":"environmental-variables/","title":"Environmental Variables","text":""},{"location":"environmental-variables/#configuring-the-engine","title":"Configuring the <code>Engine</code>","text":"<code>openai_whisper</code> <code>faster_whisper</code> <pre><code>export ASR_ENGINE=openai_whisper\n</code></pre> <pre><code>export ASR_ENGINE=faster_whisper\n</code></pre>"},{"location":"environmental-variables/#configuring-the-model","title":"Configuring the <code>Model</code>","text":"<pre><code>export ASR_MODEL=base\n</code></pre> <p>Available ASR_MODELs are <code>tiny</code>, <code>base</code>, <code>small</code>, <code>medium</code>, <code>large</code> (only OpenAI Whisper), <code>large-v1</code> and <code>large-v2</code>. Please note that <code>large</code> and <code>large-v2</code> are the same model.</p> <p>For English-only applications, the <code>.en</code> models tend to perform better, especially for the <code>tiny.en</code> and <code>base.en</code> models. We observed that the difference becomes less significant for the <code>small.en</code> and <code>medium.en</code> models.</p>"},{"location":"environmental-variables/#configuring-the-model-path","title":"Configuring the <code>Model Path</code>","text":"<pre><code>export ASR_MODEL_PATH=/data/whisper\n</code></pre>"},{"location":"licence/","title":"Licence","text":"<pre><code>MIT License\n\nCopyright (c) 2022 Ahmet Oner &amp; Besim Alibegovic\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"run/","title":"Run","text":""},{"location":"run/#usage","title":"Usage","text":"<p>Whisper ASR Webservice now available on Docker Hub. You can find the latest version of this repository on docker hub for CPU and GPU.</p> <p>Docker Hub: https://hub.docker.com/r/onerahmet/openai-whisper-asr-webservice</p> <code>CPU</code> <code>CPU (macOS)</code> <code>GPU</code> <pre><code>docker pull onerahmet/openai-whisper-asr-webservice:latest\ndocker run -d -p 9000:9000 -e ASR_MODEL=base -e ASR_ENGINE=openai_whisper onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> <p>GPU passthrough does not work on macOS due to fundamental design limitations of Docker. Docker actually runs containers within a LinuxVM on macOS. If you wish to run GPU-accelerated containers, I'm afraid Linux is your only option.</p> <p>The <code>:latest</code> image tag provides both amd64 and arm64 architectures:</p> <pre><code>docker pull onerahmet/openai-whisper-asr-webservice:latest\ndocker run -d -p 9000:9000 -e ASR_MODEL=base -e ASR_ENGINE=openai_whisper onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> <pre><code>docker pull onerahmet/openai-whisper-asr-webservice:latest-gpu\ndocker run -d --gpus all -p 9000:9000 -e ASR_MODEL=base -e ASR_ENGINE=openai_whisper onerahmet/openai-whisper-asr-webservice:latest-gpu\n</code></pre> <p>Interactive Swagger API documentation is available at http://localhost:9000/docs</p> <p></p>"},{"location":"run/#cache","title":"Cache","text":"<p>The ASR model is downloaded each time you start the container, using the large model this can take some time.  If you want to decrease the time it takes to start your container by skipping the download, you can store the cache directory (<code>~/.cache/whisper</code>) to a persistent storage.  Next time you start your container the ASR Model will be taken from the cache instead of being downloaded again.</p> <p>Important this will prevent you from receiving any updates to the models.</p> <code>Default cache dir</code> <code>With ASR_MODEL_PATH</code> <pre><code>docker run -d -p 9000:9000 -v ./yourlocaldir:~/.cache/whisper onerahmet/openai-whisper-asr-webservice:latest\n</code></pre> <pre><code>docker run -d -p 9000:9000 -e ASR_MODEL_PATH=/data/whisper -v $PWD/yourlocaldir:/data/whisper onerahmet/openai-whisper-asr-webservice:latest\n</code></pre>"}]}